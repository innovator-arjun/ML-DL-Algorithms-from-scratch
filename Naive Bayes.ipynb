{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "pd.set_option('display.max_colwidth',0)\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read the input data set\n",
    "dataset_train=pd.read_csv('Dataset/nb_arxiv_train.csv')\n",
    "test=pd.read_csv('Dataset/nb_arxiv_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The energy released in a solar flare is partitioned between thermal and\\nnon-thermal particle energy and lost to thermal conduction and radiation over a\\nbroad range of wavelengths. It is difficult to determine the conductive losses\\nand the energy radiated at transition region temperatures during the impulsive\\nphases of flares. We use UVCS measurements of O VI photons produced by 5 flares\\nand subsequently scattered by O VI ions in the corona to determine the 5.0 &lt;\\nlog T &lt; 6.0 transition region luminosities. We compare them with the rates of\\nincrease of thermal energy and the conductive losses deduced from RHESSI and\\nGOES X-ray data using areas from RHESSI images to estimate the loop volumes,\\ncross-sectional areas and scale lengths. The transition region luminosities\\nduring the impulsive phase exceed the X-ray luminosities for the first few\\nminutes, but they are smaller than the rates of increase of thermal energy\\nunless the filling factor of the X-ray emitting gas is ~ 0.01. The estimated\\nconductive losses from the hot gas are too large to be balanced by radiative\\nlosses or heating of evaporated plasma, and we conclude that the area of the\\nflare magnetic flux tubes is much smaller than the effective area measured by\\nRHESSI during this phase of the flares. For the 2002 July 23 flare, the energy\\ndeposited by non-thermal particles exceeds the X-ray and UV energy losses and\\nthe rate of increase of the thermal energy.\\n</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In light of current atmospheric neutrino oscillation data, we revisit the\\ninvisible decay of the standard model Higgs boson and other pseudoscalar mesons\\nwhich can be enhanced because of large number of KK modes in models with right\\nhanded singlet neutrinos in large extra dimensions. We find that the invisible\\ndecay rate of Higgs can be as large as $H\\to b \\bar b $ decay rate only for a\\nvery restricted region of parameter space. This parameter space is even further\\nrestricted if one demands that the dimensionless neutrino Yukawa coupling $\\l$\\nis O(1). We have also studied the scenarios where singlet neutrino propagate in\\na sub-space, which lowers the string scale $M_{\\ast}$ and keeps neutrino Yukawa\\ncoupling O(1). We have also considered decays of other spin-0 mesons to $\\nu\\n\\bar\\nu$ and found the rates to be too small for measurement.\\n</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>We consider the following basic learning task: given independent draws from\\nan unknown distribution over a discrete support, output an approximation of the\\ndistribution that is as accurate as possible in $\\ell_1$ distance (i.e. total\\nvariation or statistical distance). Perhaps surprisingly, it is often possible\\nto \"de-noise\" the empirical distribution of the samples to return an\\napproximation of the true distribution that is significantly more accurate than\\nthe empirical distribution, without relying on any prior assumptions on the\\ndistribution. We present an instance optimal learning algorithm which optimally\\nperforms this de-noising for every distribution for which such a de-noising is\\npossible. More formally, given $n$ independent draws from a distribution $p$,\\nour algorithm returns a labelled vector whose expected distance from $p$ is\\nequal to the minimum possible expected error that could be obtained by any\\nalgorithm that knows the true unlabeled vector of probabilities of distribution\\n$p$ and simply needs to assign labels, up to an additive subconstant term that\\nis independent of $p$ and goes to zero as $n$ gets large. One conceptual\\nimplication of this result is that for large samples, Bayesian assumptions on\\nthe \"shape\" or bounds on the tail probabilities of a distribution over discrete\\nsupport are not helpful for the task of learning the distribution.\\n  As a consequence of our techniques, we also show that given a set of $n$\\nsamples from an arbitrary distribution, one can accurately estimate the\\nexpected number of distinct elements that will be observed in a sample of any\\nsize up to $n \\log n$. This sort of extrapolation is practically relevant,\\nparticularly to domains such as genomics where it is important to understand\\nhow much more might be discovered given larger sample sizes, and we are\\noptimistic that our approach is practically viable.\\n</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  \\\n",
       "0  0    \n",
       "1  1    \n",
       "2  2    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Abstract  \\\n",
       "0    The energy released in a solar flare is partitioned between thermal and\\nnon-thermal particle energy and lost to thermal conduction and radiation over a\\nbroad range of wavelengths. It is difficult to determine the conductive losses\\nand the energy radiated at transition region temperatures during the impulsive\\nphases of flares. We use UVCS measurements of O VI photons produced by 5 flares\\nand subsequently scattered by O VI ions in the corona to determine the 5.0 <\\nlog T < 6.0 transition region luminosities. We compare them with the rates of\\nincrease of thermal energy and the conductive losses deduced from RHESSI and\\nGOES X-ray data using areas from RHESSI images to estimate the loop volumes,\\ncross-sectional areas and scale lengths. The transition region luminosities\\nduring the impulsive phase exceed the X-ray luminosities for the first few\\nminutes, but they are smaller than the rates of increase of thermal energy\\nunless the filling factor of the X-ray emitting gas is ~ 0.01. The estimated\\nconductive losses from the hot gas are too large to be balanced by radiative\\nlosses or heating of evaporated plasma, and we conclude that the area of the\\nflare magnetic flux tubes is much smaller than the effective area measured by\\nRHESSI during this phase of the flares. For the 2002 July 23 flare, the energy\\ndeposited by non-thermal particles exceeds the X-ray and UV energy losses and\\nthe rate of increase of the thermal energy.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "1    In light of current atmospheric neutrino oscillation data, we revisit the\\ninvisible decay of the standard model Higgs boson and other pseudoscalar mesons\\nwhich can be enhanced because of large number of KK modes in models with right\\nhanded singlet neutrinos in large extra dimensions. We find that the invisible\\ndecay rate of Higgs can be as large as $H\\to b \\bar b $ decay rate only for a\\nvery restricted region of parameter space. This parameter space is even further\\nrestricted if one demands that the dimensionless neutrino Yukawa coupling $\\l$\\nis O(1). We have also studied the scenarios where singlet neutrino propagate in\\na sub-space, which lowers the string scale $M_{\\ast}$ and keeps neutrino Yukawa\\ncoupling O(1). We have also considered decays of other spin-0 mesons to $\\nu\\n\\bar\\nu$ and found the rates to be too small for measurement.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "2    We consider the following basic learning task: given independent draws from\\nan unknown distribution over a discrete support, output an approximation of the\\ndistribution that is as accurate as possible in $\\ell_1$ distance (i.e. total\\nvariation or statistical distance). Perhaps surprisingly, it is often possible\\nto \"de-noise\" the empirical distribution of the samples to return an\\napproximation of the true distribution that is significantly more accurate than\\nthe empirical distribution, without relying on any prior assumptions on the\\ndistribution. We present an instance optimal learning algorithm which optimally\\nperforms this de-noising for every distribution for which such a de-noising is\\npossible. More formally, given $n$ independent draws from a distribution $p$,\\nour algorithm returns a labelled vector whose expected distance from $p$ is\\nequal to the minimum possible expected error that could be obtained by any\\nalgorithm that knows the true unlabeled vector of probabilities of distribution\\n$p$ and simply needs to assign labels, up to an additive subconstant term that\\nis independent of $p$ and goes to zero as $n$ gets large. One conceptual\\nimplication of this result is that for large samples, Bayesian assumptions on\\nthe \"shape\" or bounds on the tail probabilities of a distribution over discrete\\nsupport are not helpful for the task of learning the distribution.\\n  As a consequence of our techniques, we also show that given a set of $n$\\nsamples from an arbitrary distribution, one can accurately estimate the\\nexpected number of distinct elements that will be observed in a sample of any\\nsize up to $n \\log n$. This sort of extrapolation is practically relevant,\\nparticularly to domains such as genomics where it is important to understand\\nhow much more might be discovered given larger sample sizes, and we are\\noptimistic that our approach is practically viable.\\n   \n",
       "\n",
       "   Category  \n",
       "0  astro-ph  \n",
       "1  hep-ph    \n",
       "2  cs.LG     "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining All the stop words list\n",
    "stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text=text.lower() # To make the text lower\n",
    "    text=text.strip() # To remove the extra space\n",
    "    text=text.lstrip() #To remove extra space to the left of the text\n",
    "    text=text.rstrip() # To remove extra space to the right of the text\n",
    "    text=re.sub('[^a-zA-Z]',\" \",text) # To have only characters a to z\n",
    "    text = re.sub(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\",\" \",text) # To remove html links\n",
    "    text=[word for word in text.split() if word not in stop_words and len(word)>=2] # To drop one char word \n",
    "                                                                                    #and stop words\n",
    "    text=' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>energy released solar flare partitioned thermal non thermal particle energy lost thermal conduction radiation broad range wavelengths difficult determine conductive losses energy radiated transition region temperatures impulsive phases flares use uvcs measurements vi photons produced flares subsequently scattered vi ions corona determine log transition region luminosities compare rates increase thermal energy conductive losses deduced rhessi goes ray data using areas rhessi images estimate loop volumes cross sectional areas scale lengths transition region luminosities impulsive phase exceed ray luminosities first minutes smaller rates increase thermal energy unless filling factor ray emitting gas estimated conductive losses hot gas large balanced radiative losses heating evaporated plasma conclude area flare magnetic flux tubes much smaller effective area measured rhessi phase flares july flare energy deposited non thermal particles exceeds ray uv energy losses rate increase thermal energy</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>light current atmospheric neutrino oscillation data revisit invisible decay standard model higgs boson pseudoscalar mesons enhanced large number kk modes models right handed singlet neutrinos large extra dimensions find invisible decay rate higgs large bar decay rate restricted region parameter space parameter space even restricted one demands dimensionless neutrino yukawa coupling also studied scenarios singlet neutrino propagate sub space lowers string scale ast keeps neutrino yukawa coupling also considered decays spin mesons nu bar nu found rates small measurement</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>consider following basic learning task given independent draws unknown distribution discrete support output approximation distribution accurate possible ell distance total variation statistical distance perhaps surprisingly often possible de noise empirical distribution samples return approximation true distribution significantly accurate empirical distribution without relying prior assumptions distribution present instance optimal learning algorithm optimally performs de noising every distribution de noising possible formally given independent draws distribution algorithm returns labelled vector whose expected distance equal minimum possible expected error could obtained algorithm knows true unlabeled vector probabilities distribution simply needs assign labels additive subconstant term independent goes zero gets large one conceptual implication result large samples bayesian assumptions shape bounds tail probabilities distribution discrete support helpful task learning distribution consequence techniques also show given set samples arbitrary distribution one accurately estimate expected number distinct elements observed sample size log sort extrapolation practically relevant particularly domains genomics important understand much might discovered given larger sample sizes optimistic approach practically viable</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>paper characterise family finite arc transitive bicirculants show every finite arc transitive bicirculant normal cover arc transitive graph lies one eight infinite families one seven sporadic arc transitive graphs moreover basic graphs either arc transitive bicirculant arc transitive circulant graph latter case arc transitive bicirculant normal cover integer</td>\n",
       "      <td>math.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>control condensed matter systems equilibrium laser pulses allows us investigate system trajectories symmetry breaking phase transitions thus evolution collective modes single particle excitations followed diverse phase transitions femtosecond resolution present experimental observations order parameter trajectory normal superconductor transition charge density wave ordering transitions particular interest coherent evolution topological defects forming transition via kibble zurek mechanism appears measurable optical pump probe experiments experiments cdw systems reveal new phenomena coherent oscillations order parameter creation emission dispersive amplitudon modes upon annihilation topological defects mixing weakly coupled finite frequency massive bosons</td>\n",
       "      <td>cond-mat.mes-hall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  \\\n",
       "0  0    \n",
       "1  1    \n",
       "2  2    \n",
       "3  3    \n",
       "4  4    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Abstract  \\\n",
       "0  energy released solar flare partitioned thermal non thermal particle energy lost thermal conduction radiation broad range wavelengths difficult determine conductive losses energy radiated transition region temperatures impulsive phases flares use uvcs measurements vi photons produced flares subsequently scattered vi ions corona determine log transition region luminosities compare rates increase thermal energy conductive losses deduced rhessi goes ray data using areas rhessi images estimate loop volumes cross sectional areas scale lengths transition region luminosities impulsive phase exceed ray luminosities first minutes smaller rates increase thermal energy unless filling factor ray emitting gas estimated conductive losses hot gas large balanced radiative losses heating evaporated plasma conclude area flare magnetic flux tubes much smaller effective area measured rhessi phase flares july flare energy deposited non thermal particles exceeds ray uv energy losses rate increase thermal energy                                                                                                                                                                                                                                                                                                                                           \n",
       "1  light current atmospheric neutrino oscillation data revisit invisible decay standard model higgs boson pseudoscalar mesons enhanced large number kk modes models right handed singlet neutrinos large extra dimensions find invisible decay rate higgs large bar decay rate restricted region parameter space parameter space even restricted one demands dimensionless neutrino yukawa coupling also studied scenarios singlet neutrino propagate sub space lowers string scale ast keeps neutrino yukawa coupling also considered decays spin mesons nu bar nu found rates small measurement                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "2  consider following basic learning task given independent draws unknown distribution discrete support output approximation distribution accurate possible ell distance total variation statistical distance perhaps surprisingly often possible de noise empirical distribution samples return approximation true distribution significantly accurate empirical distribution without relying prior assumptions distribution present instance optimal learning algorithm optimally performs de noising every distribution de noising possible formally given independent draws distribution algorithm returns labelled vector whose expected distance equal minimum possible expected error could obtained algorithm knows true unlabeled vector probabilities distribution simply needs assign labels additive subconstant term independent goes zero gets large one conceptual implication result large samples bayesian assumptions shape bounds tail probabilities distribution discrete support helpful task learning distribution consequence techniques also show given set samples arbitrary distribution one accurately estimate expected number distinct elements observed sample size log sort extrapolation practically relevant particularly domains genomics important understand much might discovered given larger sample sizes optimistic approach practically viable   \n",
       "3  paper characterise family finite arc transitive bicirculants show every finite arc transitive bicirculant normal cover arc transitive graph lies one eight infinite families one seven sporadic arc transitive graphs moreover basic graphs either arc transitive bicirculant arc transitive circulant graph latter case arc transitive bicirculant normal cover integer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "4  control condensed matter systems equilibrium laser pulses allows us investigate system trajectories symmetry breaking phase transitions thus evolution collective modes single particle excitations followed diverse phase transitions femtosecond resolution present experimental observations order parameter trajectory normal superconductor transition charge density wave ordering transitions particular interest coherent evolution topological defects forming transition via kibble zurek mechanism appears measurable optical pump probe experiments experiments cdw systems reveal new phenomena coherent oscillations order parameter creation emission dispersive amplitudon modes upon annihilation topological defects mixing weakly coupled finite frequency massive bosons                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "\n",
       "            Category  \n",
       "0  astro-ph           \n",
       "1  hep-ph             \n",
       "2  cs.LG              \n",
       "3  math.CO            \n",
       "4  cond-mat.mes-hall  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calling the function to preprocess on train and test data \n",
    "dataset_train['Abstract']=dataset_train['Abstract'].apply(lambda x:preprocessing(x))\n",
    "test['Abstract']=test['Abstract'].apply(lambda x:preprocessing(x))\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocab={}\n",
    "        self.top_vocab=[]\n",
    "    def build_vocab(self,data):  #To build the vocabulary using the training set\n",
    "        for sentence in data:\n",
    "            token=sentence.split()\n",
    "            for words in token:  #To add the words in the vocab and maintain the occurance\n",
    "                if words not in self.vocab:  \n",
    "                    self.vocab[words]=1\n",
    "                else:\n",
    "                    self.vocab[words]+=1\n",
    "        self.top_vocab=sorted(self.vocab,key=self.vocab.get,reverse=True)[0:10000] #To get top 7500 vocab\n",
    "\n",
    "    def transform(self,data):\n",
    "        vocab_vec=(self.top_vocab)\n",
    "        res = np.zeros(shape=(len(data),len(self.top_vocab))) # Creating a dummy np array for final result\n",
    "        print(res.shape)\n",
    "        for sentence in range(0,len(data)): \n",
    "\n",
    "            text_to_vec=np.zeros(len(self.top_vocab)) # Creating a dummy vector\n",
    "\n",
    "            for words in data[sentence].split():\n",
    "                index=-1\n",
    "                try:\n",
    "                    index=vocab_vec.index(words) #creating a vector based on the top_vocab size and make it one if present\n",
    "                    text_to_vec[index]=1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            res[sentence]=text_to_vec # adding the results as every row and maintaining it in one list\n",
    "\n",
    "        return res #Return the final transformed vector\n",
    "    def fit_transform(self,data):\n",
    "        self.build_vocab(data) #To call the build vocab function\n",
    "        return self.transform(data) #To call the transform function ie to convert text to meaningful numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 10000)\n",
      "(15000, 10000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mapping ie. to convert the text to 0-14 based on the category \n",
    "\n",
    "categories=['astro-ph','astro-ph.CO','astro-ph.GA','astro-ph.SR','cond-mat.mes-hall','cond-mat.mtrl-sci',\n",
    "           'cs.LG','gr-qc','hep-ph','hep-th','math.AP','math.CO','physics.optics','physics.optics','stat.ML']\n",
    "\n",
    "mapping_output={'astro-ph':0,'astro-ph.CO':1,'astro-ph.GA':2,'astro-ph.SR':3,'cond-mat.mes-hall':4,\n",
    "                'cond-mat.mtrl-sci':5,'cs.LG':6,'gr-qc':7,'hep-ph':8,'hep-th':9,'math.AP':10\n",
    "                ,'math.CO':11,'physics.optics':12,'quant-ph':13,'stat.ML':14}\n",
    "dataset_train['Category'].replace(mapping_output, inplace=True)\n",
    "\n",
    "vec=Vectorizer()\n",
    "X=vec.fit_transform(dataset_train['Abstract'])\n",
    "\n",
    "\n",
    "y=dataset_train['Category'].to_numpy()\n",
    "\n",
    "\n",
    "X_sep_test=vec.transform(test['Abstract'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Created a random split function, that splits X & y as train and test\n",
    "random.seed(30)\n",
    "def split(X,y,p):\n",
    "    full_li=list(range(0,int(X.shape[0])))\n",
    "    random.shuffle(full_li)\n",
    "    X_train=X[full_li[0:int(p*X.shape[0])]]\n",
    "    y_train=y[full_li[0:int(p*X.shape[0])]]\n",
    "    X_test=X[full_li[int(p*X.shape[0]):]]\n",
    "    y_test=y[full_li[int(p*X.shape[0]):]]\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the split function\n",
    "X_train,y_train,X_test,y_test=split(X,y,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a function to print the accuracy and the confusion matrix to analyse the result\n",
    "y_unique_len=len(list(mapping_output.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_loss(y_test,predictions):\n",
    "    matrix=np.zeros((y_unique_len,y_unique_len))\n",
    "    for true,pred in zip(y_test,predictions):  #creates a confusion matrix\n",
    "        matrix[int(true)-1][int(pred)-1]+=1\n",
    "\n",
    "    matrix = matrix.astype(int)\n",
    "    print(matrix)\n",
    "\n",
    "    print(np.sum(np.diag(matrix))) # To calculate the correct prediction\n",
    "    accuracy=np.sum(np.diag(matrix))/len(predictions) #To calculate the accuracy\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BernoulliNaive:\n",
    "    def __init(self):\n",
    "        pass\n",
    "    def fit(self,X,y):\n",
    "        #Calculating class wise occurance based on the over all category\n",
    "        self.n_classes=len(np.unique(y))\n",
    "        \n",
    "        self.counts=np.zeros(self.n_classes)\n",
    "        for i in y:\n",
    "            self.counts[i]+=1\n",
    "        self.counts/=len(y)\n",
    "        \n",
    "        self.params=np.zeros((self.n_classes,X.shape[1]))\n",
    "        #nfeatures*nclasses\n",
    "        for index in range(len(X)):\n",
    "            self.params[y[index]]+=X[index]\n",
    "        self.params+=1  # Using Laplace , which is very important since we are finding the product\n",
    "        class_sums=self.params.sum(axis=1)+self.n_classes\n",
    "        self.params=self.params/class_sums[:,np.newaxis]\n",
    "    def predict(self,X):\n",
    "        neg_prob=np.log(1-self.params)\n",
    "        res=np.dot(X,(np.log(self.params)-neg_prob).T)\n",
    "        res+=np.log(self.counts)+neg_prob.sum(axis=1)\n",
    "        return np.argmax(res,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the Bernoulli Class and use the function fit with train data\n",
    "clf=BernoulliNaive()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test) # to predict based on the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 79  51   0   1   0   0   8   1   0   0   0   0   0   2   4]\n",
      " [ 21 108   6   0   0   0   0   0   0   0   0   0   0   0   4]\n",
      " [  0  15 111   0   0   1   1   0   0   0   0   0   0   0   8]\n",
      " [  0   0   0 120  15   0   0   0   2   0   0   6   4   0   0]\n",
      " [  0   1   1  25 119   0   1   0   0   1   0   4   0   0   0]\n",
      " [  0   0   0   0   0 115   0   0   0   1   1   0   0  36   0]\n",
      " [  7   0   2   0   0   1 132   1  18   1   0   2   1   0   1]\n",
      " [  1   0   0   0   1   1   2 141   6   0   0   0   1   2   1]\n",
      " [  0   0   0   2   0   0  18   7 131   2   0   0   2   0   0]\n",
      " [  0   0   0   0   1   0   1   0   1 145   0   0   1   1   0]\n",
      " [  0   0   1   0   0   1   0   0   0   1 128   0   0   1   0]\n",
      " [  0   0   0   5   1   0   2   0   0   0   0 144   1   1   0]\n",
      " [  0   0   0   7   0   4   4   0   2   0   4  13 108   0   0]\n",
      " [  0   0   0   0   0  24   0   0   1   0   1   0   0 138   0]\n",
      " [ 37  40  19   0   1   0   3   0   0   1   0   1   1   1  47]]\n",
      "1766\n",
      "0.7848888888888889\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc=confusion_loss(y_test,predictions)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
